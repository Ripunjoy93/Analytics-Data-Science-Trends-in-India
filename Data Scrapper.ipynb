{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping Naukri's Data Science/Analytics Jobs data\n",
    "\n",
    "**Python code using ```BeautifulSoup``` to scrap data**     \n",
    "Motive is to find recent demands in data science in India as well as to important insights such as salary, experience, tools etc. related to data science job profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import _pickle as cPickle\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import OrderedDict\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT\n",
    "1. Going to Query Job by different KeyWords Like Data Scientist, Data Analyst, Business Analyst, Machine Learning etc.\n",
    "1. Though there is many pages for each key-word, after 20-25 pages I am noticing that Jobs are not as data scientist or the keyword, they matched because of the some portion of key word like (data-scientist-analyst-business-machine-learning) mathced.\n",
    "1. So for different keywords, instead of getting all the pages data, I will simply get first 20-30 page of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the data frame every time before starting the scrap job\n",
    "job_df = pd.DataFrame()\n",
    "\n",
    "# df to validate with the un-scraped links from job_df\n",
    "job_links_df = pd.DataFrame()\n",
    "\n",
    "# Motive is to get all the data science jobs from \"https://www.naukri.com\"\n",
    "\n",
    "# URLS : \"-\" is added to help later while structuring the other pages URL\n",
    "data_scientist = \"https://www.naukri.com/data-scientist-jobs-\"\n",
    "machine_learning = \"https://www.naukri.com/machine-learning-jobs-\"\n",
    "data_analyst = \"https://www.naukri.com/data-analyst-jobs-\"\n",
    "# go till page 60\n",
    "business_analyst = \"https://www.naukri.com/business-analyst-jobs-\"\n",
    "\n",
    "# Base URL (You can write a for loop to iterate through the above URLs, I am putting it manually so that server doesn't receive lots \n",
    "# of hits at a time)\n",
    "base_url = business_analyst\n",
    "base_response = requests.get(base_url)\n",
    "base_page = base_response.text\n",
    "\n",
    "# Convert the response to BeautifulSoup object\n",
    "base_soup = BeautifulSoup(base_page, \"html.parser\")\n",
    "\n",
    "# Find the total number of jobs : which is 10043\n",
    "num_jobs = int(base_soup.find(\"div\", { \"class\" : \"count\" }).h1.contents[1].getText().split(' ')[-1])\n",
    "# Each page lists 50 jobs, so total pages\n",
    "num_pages = int(math.ceil(num_jobs/50.0))\n",
    "\n",
    "# Get all the job links, each page have 50 job links\n",
    "# pattern in page url is - https://www.naukri.com/data-scientist-jobs-page_number\n",
    "\n",
    "# Create a empty list to store all job links\n",
    "job_links = []\n",
    "\n",
    "# Want to scrap in a gap, so that I don't overload there server :: 30 for others, 60 for business_analyst, 40 for data_scientist\n",
    "req_page = 60\n",
    "# keep changing the start_ind\n",
    "start_ind = 1\n",
    "end_ind = start_ind + req_page\n",
    "\n",
    "# description labels (other informations about the job)\n",
    "labels = ['Salary', 'Industry', 'Functional_Area', 'Role_Category', 'Design_Role']\n",
    "# education requirements\n",
    "edu_labels = [\"UG\", \"PG\", \"Doctorate\"]\n",
    "# For loop to get link from each of num_pages\n",
    "# To run in a single loop USE : range(1, num_pages+1)\n",
    "for page in range(start_ind, end_ind):\n",
    "    # structuring the page URL\n",
    "    page_url = base_url + str(page)\n",
    "    page_response = requests.get(page_url)\n",
    "    page_txt = page_response.text\n",
    "    page_soup = BeautifulSoup(page_txt, \"html.parser\")\n",
    "    # 50 job links are in the class content, so filtering only content class\n",
    "    links = [l.get(\"href\") for l in page_soup.find_all(\"a\", {\"class\":\"content\"})]\n",
    "    # Append the links into job_links\n",
    "    #for pl in links:\n",
    "        #job_links.append(pl)\n",
    "    for job_url in links:\n",
    "        job_response = requests.get(job_url)\n",
    "        job_page = job_response.text\n",
    "        job_soup = BeautifulSoup(job_page, \"html.parser\")\n",
    "        # get one dataframe with all the links and response so that we can later check from which link we are not able to get data\n",
    "        jdf = OrderedDict({\"Job_Link\":job_url, \"Response\":str(job_response)})\n",
    "        job_links_df = job_links_df.append(jdf,ignore_index=True)\n",
    "        try:\n",
    "            job_title = job_soup.find(\"h1\", {\"itemprop\":\"title\"}).getText().strip()\n",
    "            company_name = job_soup.find(\"a\",{\"itemprop\":\"hiringOrganization\"}).getText().strip()\n",
    "            experience = job_soup.find(\"span\",{\"itemprop\":\"experienceRequirements\"}).getText().strip()\n",
    "            location = job_soup.find(\"div\",{\"class\":\"loc\"}).getText().strip()\n",
    "            salary = job_soup.find(\"span\",{\"class\":\"sal\"}).getText().strip()\n",
    "            #openings = job_soup.find(\"div\",{\"class\":\"sumFoot\"}).find_all(\"span\")\n",
    "            #openings\n",
    "            num_openings = \"\"\n",
    "            job_post = \"\"\n",
    "            for x in job_soup.find(\"div\",{\"class\":\"sumFoot\"}).find_all(\"span\"):\n",
    "                if \"Openings\" in x.text.strip():\n",
    "                    num_openings = x.text.strip()\n",
    "                if \"Posted\" in x.text.strip():\n",
    "                    job_post = x.text.strip()\n",
    "            job_application = job_soup.find(\"span\",{\"class\":\"jApplys\"}).find(\"strong\").getText().strip()\n",
    "            job_view = job_soup.find(\"span\",{\"class\":\"jViews\"}).find(\"strong\").getText().strip()\n",
    "            job_description = job_soup.find(\"ul\",{\"itemprop\":\"description\"}).getText().strip()\n",
    "            # description labels (other informations about the job)\n",
    "            #for x in job_soup.find(\"div\",{\"class\":\"jDisc mt20\"}).contents:\n",
    "            #    if len(str(x).replace(' ',''))!=0 :\n",
    "            #        print(x.getText().split(':')[-1].strip())\n",
    "            other_info = [x.getText().split(':')[-1].strip() for x in job_soup.find(\"div\",{\"class\":\"jDisc mt20\"}).contents if len(str(x).replace(' ',''))!=0]\n",
    "            other_info_label = {labels: other_info for labels, other_info in zip(labels, other_info)}\n",
    "            key_skills = ','.join(job_soup.find(\"div\",{\"class\":\"ksTags\"}).getText().split(\"  \"))[1:]\n",
    "            skill_experience = job_soup.find(\"ul\",{\"class\":\"listing mt15\"}).getText().strip()\n",
    "            # putting the education information\n",
    "            education = [x.getText().split(':') for x in job_soup.find(\"div\",{\"itemprop\":\"educationRequirements\"}).contents if len(str(x).replace(' ',''))!=0]\n",
    "            education_info = {edu_label.strip(): education.strip() for edu_label, education in education}\n",
    "            for l in edu_labels:\n",
    "                if l not in education_info.keys():\n",
    "                    education_info[l] = \"\"\n",
    "            # recruiter information\n",
    "            # This is inside a javascript button : BeautifulSoup is HTML parser: For this need SELENIUM\n",
    "        except AttributeError:\n",
    "            continue\n",
    "        df = OrderedDict({'Job_Link':job_url, 'Job_Title':job_title, 'Company_Name':company_name, 'Experience':experience, 'Location':location, 'SalaryI':salary, 'Num_Openings':num_openings,\n",
    "                          'Job_Post':job_post, 'Job_Application':job_application,\n",
    "                          'Job_View':job_view, 'Key_Skills':key_skills, 'Skill_Experience':skill_experience})\n",
    "        df.update(other_info_label)\n",
    "        df.update(education_info)\n",
    "        job_df = job_df.append(df,ignore_index=True)\n",
    "        print(job_df.shape)\n",
    "        time.sleep(1)\n",
    "    print(\"page\" + str(page))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we need to pay attention that job can repeat in searches because of different keywords, so need to remove the duplicate based on job_url, company_name, job_titel etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving the data in pkl format\n",
    "import _pickle as cPickle\n",
    "# Python 3, _pickle\n",
    "column_names = ['Job_Link', 'Job_Title', 'Company_Name', 'Experience', 'Location', 'SalaryI', 'Num_Openings', 'Job_Post', 'Job_Application',\n",
    "                'Job_View', 'Salary', 'Industry', 'Functional_Area', 'Role_Category', 'Design_Role', 'Key_Skills', 'Skill_Experience', \"UG\",\n",
    "                \"PG\", \"Doctorate\"]\n",
    "\n",
    "job_df= job_df.reindex(columns=column_names)        \n",
    "with open('data/job_df_business_analyst.pkl', 'wb') as f:\n",
    "    cPickle.dump(job_df, f)\n",
    "\n",
    "# Save the scraped links\n",
    "with open(\"data/job_links_df_business_analyst.pkl\", \"wb\") as f:\n",
    "    cPickle.dump(job_links_df, f)\n",
    "\n",
    "# Read canned scraped links\n",
    "# Some error here ?\n",
    "#with open('job_df.pkl', 'r') as f:\n",
    "#    job_df = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save data as csv\n",
    "job_df.to_csv(\"data/job_df_business_analyst.csv\", encoding='utf8')\n",
    "job_links_df.to_csv(\"data/job_links_df_business_analyst.csv\", encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "1. There are many links which we were not able to execute to get the data, we will filter out those links and run the scrap job again based on the un-executed links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 shape(719, 21)\n",
      "df2 shape(678, 21)\n",
      "df3 shape(754, 21)\n",
      "df4 shape(1072, 21)\n",
      "df4 shape(1072, 21)\n",
      "df5 shape(1216, 21)\n",
      "job_df shape(4439, 20)\n",
      "job_df shape after removing duplicates(3727, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Link</th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Location</th>\n",
       "      <th>SalaryI</th>\n",
       "      <th>Num_Openings</th>\n",
       "      <th>Job_Post</th>\n",
       "      <th>Job_Application</th>\n",
       "      <th>Job_View</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Functional_Area</th>\n",
       "      <th>Role_Category</th>\n",
       "      <th>Design_Role</th>\n",
       "      <th>Key_Skills</th>\n",
       "      <th>Skill_Experience</th>\n",
       "      <th>UG</th>\n",
       "      <th>PG</th>\n",
       "      <th>Doctorate</th>\n",
       "      <th>Actual_Job_Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.naukri.com/job-listings-Data-Scien...</td>\n",
       "      <td>Data Scientist - Perl/python</td>\n",
       "      <td>niki.ai</td>\n",
       "      <td>3 - 6 yrs</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Not Disclosed by Recruiter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Posted Just Now</td>\n",
       "      <td>Less than 10</td>\n",
       "      <td>Less than 10</td>\n",
       "      <td>Not Disclosed by Recruiter</td>\n",
       "      <td>IT-Software  /    Software Services</td>\n",
       "      <td>Analytics &amp; Business Intelligence</td>\n",
       "      <td>Analytics &amp; BI</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Machine Learning,Python,Data Analysis,Statisti...</td>\n",
       "      <td>Qualifications and Skills :1. B.tech/MS or equ...</td>\n",
       "      <td>B.Tech/B.E. - Any Specialization</td>\n",
       "      <td>M.Tech - Any Specialization, MS/M.Sc(Science) ...</td>\n",
       "      <td>Doctorate Not Required</td>\n",
       "      <td>https://www.naukri.com/job-listings-Data-Scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.naukri.com/job-listings-Data-Scien...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Brillio Technologies Pvt. Ltd</td>\n",
       "      <td>2 - 5 yrs</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Not Disclosed by Recruiter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Posted 1 day ago</td>\n",
       "      <td>14</td>\n",
       "      <td>166</td>\n",
       "      <td>Not Disclosed by Recruiter</td>\n",
       "      <td>IT-Software  /    Software Services</td>\n",
       "      <td>Medical   ,     Healthcare   ,     R&amp;D   ,    ...</td>\n",
       "      <td>R&amp;D</td>\n",
       "      <td>Research Scientist</td>\n",
       "      <td>Analytics,Data analysis,Python,Visualization,A...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Any Graduate - Any Specialization</td>\n",
       "      <td>Post Graduation Not Required</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.naukri.com/job-listings-Data-Scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.naukri.com/job-listings-Senior-Dat...</td>\n",
       "      <td>Senior Data Scientist, Data Scientist,</td>\n",
       "      <td>Knorex India</td>\n",
       "      <td>3 - 6 yrs</td>\n",
       "      <td>Pune(Hadapsar)</td>\n",
       "      <td>4,25,000 - 9,25,000 P.A.</td>\n",
       "      <td>Openings: 1</td>\n",
       "      <td>Posted Just Now</td>\n",
       "      <td>Less than 10</td>\n",
       "      <td>Less than 10</td>\n",
       "      <td>INR  4,25,000 - 9,25,000 P.A.</td>\n",
       "      <td>IT-Software  /    Software Services</td>\n",
       "      <td>IT Software - System Programming</td>\n",
       "      <td>Programming &amp; Design</td>\n",
       "      <td>System Analyst</td>\n",
       "      <td>python,machine learning,r,algorithms,java,mark...</td>\n",
       "      <td>Please refer to the Job description above</td>\n",
       "      <td>B.Tech/B.E. - Computers</td>\n",
       "      <td>M.Tech - Computers, Post Graduation Not Required</td>\n",
       "      <td>Doctorate Not Required</td>\n",
       "      <td>https://www.naukri.com/job-listings-Senior-Dat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Job_Link                               Job_Title                   Company_Name Experience        Location                     SalaryI Num_Openings          Job_Post Job_Application      Job_View                         Salary                             Industry                                    Functional_Area         Role_Category         Design_Role                                         Key_Skills  \\\n",
       "0  https://www.naukri.com/job-listings-Data-Scien...            Data Scientist - Perl/python                        niki.ai  3 - 6 yrs       Bengaluru  Not Disclosed by Recruiter          NaN   Posted Just Now    Less than 10  Less than 10     Not Disclosed by Recruiter  IT-Software  /    Software Services                  Analytics & Business Intelligence        Analytics & BI        Data Analyst  Machine Learning,Python,Data Analysis,Statisti...   \n",
       "1  https://www.naukri.com/job-listings-Data-Scien...                          Data Scientist  Brillio Technologies Pvt. Ltd  2 - 5 yrs       Bengaluru  Not Disclosed by Recruiter          NaN  Posted 1 day ago              14           166     Not Disclosed by Recruiter  IT-Software  /    Software Services  Medical   ,     Healthcare   ,     R&D   ,    ...                   R&D  Research Scientist  Analytics,Data analysis,Python,Visualization,A...   \n",
       "2  https://www.naukri.com/job-listings-Senior-Dat...  Senior Data Scientist, Data Scientist,                   Knorex India  3 - 6 yrs  Pune(Hadapsar)    4,25,000 - 9,25,000 P.A.  Openings: 1   Posted Just Now    Less than 10  Less than 10  INR  4,25,000 - 9,25,000 P.A.  IT-Software  /    Software Services                   IT Software - System Programming  Programming & Design      System Analyst  python,machine learning,r,algorithms,java,mark...   \n",
       "\n",
       "                                    Skill_Experience                                 UG                                                 PG               Doctorate                                    Actual_Job_Link  \n",
       "0  Qualifications and Skills :1. B.tech/MS or equ...   B.Tech/B.E. - Any Specialization  M.Tech - Any Specialization, MS/M.Sc(Science) ...  Doctorate Not Required  https://www.naukri.com/job-listings-Data-Scien...  \n",
       "1                                                NaN  Any Graduate - Any Specialization                       Post Graduation Not Required                     NaN  https://www.naukri.com/job-listings-Data-Scien...  \n",
       "2          Please refer to the Job description above            B.Tech/B.E. - Computers   M.Tech - Computers, Post Graduation Not Required  Doctorate Not Required  https://www.naukri.com/job-listings-Senior-Dat...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we saved the parsed data in various file, so lets get all the data for job\n",
    "\n",
    "df1 = pd.read_csv(\"data/job_df20.csv\", encoding = \"ISO-8859-1\")\n",
    "# Using different encoding because did some mistake while saving the first data file, so not able to read as UTF-8\n",
    "print(\"df1 shape\" + str(df1.shape))\n",
    "df2 = pd.read_csv(\"data/job_df40.csv\", encoding = \"utf-8\")\n",
    "print(\"df2 shape\" + str(df2.shape))\n",
    "df3 = pd.read_csv(\"data/job_df_dataAnalyst.csv\", encoding = \"utf-8\")\n",
    "print(\"df3 shape\" + str(df3.shape))\n",
    "df4 = pd.read_csv(\"data/job_df_machine.csv\", encoding = \"utf-8\")\n",
    "print(\"df4 shape\" + str(df4.shape))\n",
    "df5 = pd.read_csv(\"data/job_df_business_analyst.csv\", encoding = \"utf-8\")\n",
    "print(\"df5 shape\" + str(df5.shape))\n",
    "\n",
    "# Append all the df's to get one big job df\n",
    "job_df = df1.append(df2).append(df3).append(df4).append(df5)\n",
    "\n",
    "# There is an Unnamed:0 column, droping that column\n",
    "job_df.drop(job_df.columns[[0]], axis=1, inplace=True)\n",
    "print(\"data/job_df shape\" + str(job_df.shape))\n",
    "\n",
    "# Dropping the duplicate Job_URL rows from the data frame so that it comes only once\n",
    "# In Job_Links, the last \"?src\" parts changes dynamically, so need to modify/remove that part to get the job_links\n",
    "# So that we can compare the duplicates\n",
    "job_df[[\"Actual_Job_Link\", \"SearchId\"]] = job_df.Job_Link.str.split(\"?\" , expand=True)\n",
    "# drop SearchId column (starts with src- doesn't have any importance)\n",
    "job_df.drop(\"SearchId\", axis=1, inplace=True)\n",
    "job_df = job_df.drop_duplicates(\"Actual_Job_Link\",keep=\"first\")\n",
    "print(\"job_df shape after removing duplicates\" + str(job_df.shape))\n",
    "job_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jdf1 shape(1001, 3)\n",
      "jdf2 shape(1001, 3)\n",
      "jdf3 shape(1500, 3)\n",
      "jdf4 shape(1500, 3)\n",
      "jdf5 shape(2999, 3)\n",
      "job_links shape(8001, 2)\n",
      "job_links shape after removing duplicates(6015, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Link</th>\n",
       "      <th>Response</th>\n",
       "      <th>Actual_Job_Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.naukri.com/job-listings-AI-Scienti...</td>\n",
       "      <td>&lt;Response [200]&gt;</td>\n",
       "      <td>https://www.naukri.com/job-listings-AI-Scienti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.naukri.com/job-listings-Data-Scien...</td>\n",
       "      <td>&lt;Response [200]&gt;</td>\n",
       "      <td>https://www.naukri.com/job-listings-Data-Scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.naukri.com/job-listings-Data-Scien...</td>\n",
       "      <td>&lt;Response [200]&gt;</td>\n",
       "      <td>https://www.naukri.com/job-listings-Data-Scien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Job_Link          Response                                    Actual_Job_Link\n",
       "0  https://www.naukri.com/job-listings-AI-Scienti...  <Response [200]>  https://www.naukri.com/job-listings-AI-Scienti...\n",
       "1  https://www.naukri.com/job-listings-Data-Scien...  <Response [200]>  https://www.naukri.com/job-listings-Data-Scien...\n",
       "2  https://www.naukri.com/job-listings-Data-Scien...  <Response [200]>  https://www.naukri.com/job-listings-Data-Scien..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all the data for job_links: combination of links, most of them we scrapped and available in job_df data set\n",
    "# Due to various reason, some of them we are not able to scrapped\n",
    "jdf1 = pd.read_csv(\"data/job_links_df20.csv\", encoding = \"ISO-8859-1\")\n",
    "print(\"jdf1 shape\" + str(jdf1.shape))\n",
    "jdf2 = pd.read_csv(\"data/job_links_df40.csv\", encoding = \"utf-8\")\n",
    "print(\"jdf2 shape\" + str(jdf2.shape))\n",
    "jdf3 = pd.read_csv(\"data/job_links_df_dataAnalyst.csv\", encoding = \"utf-8\")\n",
    "print(\"jdf3 shape\" + str(jdf3.shape))\n",
    "jdf4 = pd.read_csv(\"data/job_links_df_machine.csv\", encoding = \"utf-8\")\n",
    "print(\"jdf4 shape\" + str(jdf4.shape))\n",
    "jdf5 = pd.read_csv(\"data/job_links_df_business_analyst.csv\", encoding = \"utf-8\")\n",
    "print(\"jdf5 shape\" + str(jdf5.shape))\n",
    "\n",
    "# Append into a big df\n",
    "job_links = jdf1.append(jdf2).append(jdf3).append(jdf4).append(jdf5)\n",
    "\n",
    "# There is an Unnamed:0 column, droping that column\n",
    "job_links.drop(job_links.columns[[0]], axis=1, inplace=True)\n",
    "print(\"job_links shape\" + str(job_links.shape))\n",
    "\n",
    "# Dropping the duplicate Job_URL rows from the data frame so that it comes only once\n",
    "# In Job_Links, the last \"?src\" parts changes dynamically, so need to modify/remove that part to get the job_links\n",
    "# So that we can compare the duplicates\n",
    "job_links[['Actual_Job_Link', 'SearchId']] = job_links.Job_Link.str.split(\"?\" , expand=True)\n",
    "# drop SearchId column\n",
    "job_links.drop(\"SearchId\", axis=1, inplace=True)\n",
    "job_links = job_links.drop_duplicates(\"Actual_Job_Link\")\n",
    "print(\"job_links shape after removing duplicates\" + str(job_links.shape))\n",
    "job_links.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2928, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clearly we can notice that there are unique job_links for whom we don't have any data\n",
    "# We will try to get those links and pass it to the scrapper to scrap those links again,\n",
    "# or try to modify the scrapping operation if required\n",
    "\n",
    "# Getting the links for whom we have data\n",
    "links_with_data = job_df['Actual_Job_Link'].to_frame()\n",
    "# Getting all unique job links\n",
    "all_job_links = job_links['Actual_Job_Link'].to_frame()\n",
    "# ANTI-JOIN is neede: get all the links which are available in all_job_links but not in links_with_data\n",
    "out_merge = pd.merge(links_with_data,all_job_links, how='outer', indicator=True)\n",
    "remaining_job_list = out_merge[out_merge['_merge'] == 'right_only']\n",
    "remaining_job_list.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are 2928 links, for which we can try again to scrap.**    \n",
    "*From few of them, we will be able to get data, and remaining will be because of different page structure.\n",
    "We are not going to spend time to get those data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "# Initialize a new df\n",
    "job_rem_df = pd.DataFrame()\n",
    "\n",
    "# description labels (other informations about the job)\n",
    "labels = ['Salary', 'Industry', 'Functional_Area', 'Role_Category', 'Design_Role']\n",
    "# education requirements\n",
    "edu_labels = [\"UG\", \"PG\", \"Doctorate\"]\n",
    "\n",
    "# Iterate through all the links in remaining_job_list\n",
    "for index, row in remaining_job_list.iterrows():\n",
    "    job_url = row[\"Actual_Job_Link\"]\n",
    "    job_response = requests.get(job_url)\n",
    "    job_page = job_response.text\n",
    "    job_soup = BeautifulSoup(job_page, \"html.parser\")\n",
    "    try:\n",
    "        job_title = job_soup.find(\"h1\", {\"itemprop\":\"title\"}).getText().strip()\n",
    "        company_name = job_soup.find(\"a\",{\"itemprop\":\"hiringOrganization\"}).getText().strip()\n",
    "        experience = job_soup.find(\"span\",{\"itemprop\":\"experienceRequirements\"}).getText().strip()\n",
    "        location = job_soup.find(\"div\",{\"class\":\"loc\"}).getText().strip()\n",
    "        salary = job_soup.find(\"span\",{\"class\":\"sal\"}).getText().strip()\n",
    "        #openings = job_soup.find(\"div\",{\"class\":\"sumFoot\"}).find_all(\"span\")\n",
    "        #openings\n",
    "        num_openings = \"\"\n",
    "        job_post = \"\"\n",
    "        for x in job_soup.find(\"div\",{\"class\":\"sumFoot\"}).find_all(\"span\"):\n",
    "            if \"Openings\" in x.text.strip():\n",
    "                num_openings = x.text.strip()\n",
    "            if \"Posted\" in x.text.strip():\n",
    "                job_post = x.text.strip()\n",
    "        job_application = job_soup.find(\"span\",{\"class\":\"jApplys\"}).find(\"strong\").getText().strip()\n",
    "        job_view = job_soup.find(\"span\",{\"class\":\"jViews\"}).find(\"strong\").getText().strip()\n",
    "        job_description = job_soup.find(\"ul\",{\"itemprop\":\"description\"}).getText().strip()\n",
    "        # description labels (other informations about the job)\n",
    "        #for x in job_soup.find(\"div\",{\"class\":\"jDisc mt20\"}).contents:\n",
    "        #    if len(str(x).replace(' ',''))!=0 :\n",
    "        #        print(x.getText().split(':')[-1].strip())\n",
    "        other_info = [x.getText().split(':')[-1].strip() for x in job_soup.find(\"div\",{\"class\":\"jDisc mt20\"}).contents if len(str(x).replace(' ',''))!=0]\n",
    "        other_info_label = {labels: other_info for labels, other_info in zip(labels, other_info)}\n",
    "        key_skills = ','.join(job_soup.find(\"div\",{\"class\":\"ksTags\"}).getText().split(\"  \"))[1:]\n",
    "        skill_experience = job_soup.find(\"ul\",{\"class\":\"listing mt15\"}).getText().strip()\n",
    "        # putting the education information\n",
    "        education = [x.getText().split(':') for x in job_soup.find(\"div\",{\"itemprop\":\"educationRequirements\"}).contents if len(str(x).replace(' ',''))!=0]\n",
    "        education_info = {edu_label.strip(): education.strip() for edu_label, education in education}\n",
    "        for l in edu_labels:\n",
    "            if l not in education_info.keys():\n",
    "                education_info[l] = \"\"\n",
    "        # recruiter information\n",
    "        # This is inside a javascript button : BeautifulSoup is HTML parser: For this need SELENIUM\n",
    "    except AttributeError:\n",
    "        print(\"Attribute Error\")\n",
    "        continue\n",
    "    df = OrderedDict({'Job_Link':job_url, 'Job_Title':job_title, 'Company_Name':company_name, 'Experience':experience, 'Location':location, \n",
    "                      'SalaryI':salary, 'Num_Openings':num_openings, 'Job_Post':job_post, 'Job_Application':job_application, 'Job_View':job_view, \n",
    "                      'Key_Skills':key_skills, 'Skill_Experience':skill_experience})\n",
    "    df.update(other_info_label)\n",
    "    df.update(education_info)\n",
    "    job_rem_df = job_rem_df.append(df,ignore_index=True)\n",
    "    print(job_rem_df.shape)\n",
    "    time.sleep(1)\n",
    "print(\"Iteration Completed and get data of\" + str(job_rem_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 21)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicating the Job_Link as Actual_Job_Link to make the shape of all dataframes same\n",
    "job_rem_df[\"Actual_Job_Link\"] = job_rem_df[\"Job_Link\"]\n",
    "job_rem_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Out of 2928, we got only 20\n",
    "# Append this with job_df\n",
    "job_df = job_df.append(job_rem_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2908, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Structure of some page changed, as well as attribute error in try block,\n",
    "# we will do two things, first by if else, we will handle the error for same structured pages\n",
    "# and modify the scrap query for different structure pages\n",
    "# Out of 2928\n",
    "# Getting the links for whom we have data\n",
    "rem_links_with_data = job_rem_df['Actual_Job_Link'].to_frame()\n",
    "# Getting all remaining unique job links used previously\n",
    "remaining_job_list = remaining_job_list['Actual_Job_Link'].to_frame()\n",
    "# ANTI-JOIN is neede: get all the links which are available in all_job_links but not in links_with_data\n",
    "ot_merge = pd.merge(rem_links_with_data,remaining_job_list, how='outer', indicator=True)\n",
    "rem_job_list = ot_merge[ot_merge['_merge'] == 'right_only']\n",
    "rem_job_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_df.to_csv(\"updated_job.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Completed and get data of(2908, 20)\n"
     ]
    }
   ],
   "source": [
    "# 2928 - 20 = 2908, remaning links\n",
    "# first using if else condition\n",
    "\n",
    "# Initialize a new df\n",
    "job_rem_df1 = pd.DataFrame()\n",
    "\n",
    "# description labels (other informations about the job)\n",
    "labels = ['Salary', 'Industry', 'Functional_Area', 'Role_Category', 'Design_Role']\n",
    "# education requirements\n",
    "edu_labels = [\"UG\", \"PG\", \"Doctorate\"]\n",
    "\n",
    "# Iterate through all the links in remaining_job_list\n",
    "for index, row in rem_job_list.iterrows():\n",
    "    job_url = row[\"Actual_Job_Link\"]\n",
    "    job_response = requests.get(job_url)\n",
    "    job_page = job_response.text\n",
    "    job_soup = BeautifulSoup(job_page, \"html.parser\")    \n",
    "    #try:\n",
    "    if job_soup.find(\"h1\", {\"itemprop\":\"title\"}) is None:\n",
    "        job_title = \"Error\"\n",
    "    else:\n",
    "        job_title = job_soup.find(\"h1\", {\"itemprop\":\"title\"}).getText().strip()\n",
    "    if job_soup.find(\"a\",{\"itemprop\":\"hiringOrganization\"}) is None:\n",
    "        company_name = \"Error\"\n",
    "    else:\n",
    "        company_name = job_soup.find(\"a\",{\"itemprop\":\"hiringOrganization\"}).getText().strip()\n",
    "    if job_soup.find(\"span\",{\"itemprop\":\"experienceRequirements\"}) is None:\n",
    "        experience = \"Error\"\n",
    "    else:\n",
    "        experience = job_soup.find(\"span\",{\"itemprop\":\"experienceRequirements\"}).getText().strip()\n",
    "    if job_soup.find(\"div\",{\"class\":\"loc\"}) is None:\n",
    "        location = \"Error\"\n",
    "    else:\n",
    "        location = job_soup.find(\"div\",{\"class\":\"loc\"}).getText().strip()\n",
    "    if job_soup.find(\"span\",{\"class\":\"sal\"}) is None:\n",
    "        salary = \"Error\"\n",
    "    else:\n",
    "        salary = job_soup.find(\"span\",{\"class\":\"sal\"}).getText().strip()\n",
    "    #openings = job_soup.find(\"div\",{\"class\":\"sumFoot\"}).find_all(\"span\")\n",
    "    #openings\n",
    "    num_openings = \"\"\n",
    "    job_post = \"\"\n",
    "    if job_soup.find(\"div\",{\"class\":\"sumFoot\"}) is None:\n",
    "        num_openings = \"Error\"\n",
    "        job_post = \"Error\"\n",
    "    else:\n",
    "        if job_soup.find(\"div\",{\"class\":\"sumFoot\"}).find_all(\"span\") is None:\n",
    "            num_openings = \"Error\"\n",
    "            job_post = \"Error\"\n",
    "        else:\n",
    "            for x in job_soup.find(\"div\",{\"class\":\"sumFoot\"}).find_all(\"span\"):\n",
    "                if \"Openings\" in x.text.strip():\n",
    "                    num_openings = x.text.strip()\n",
    "                if \"Posted\" in x.text.strip():\n",
    "                    job_post = x.text.strip()\n",
    "    if job_soup.find(\"span\",{\"class\":\"jApplys\"}) is None:\n",
    "        job_application = \"Error\"\n",
    "    else:\n",
    "        if job_soup.find(\"span\",{\"class\":\"jApplys\"}).find(\"strong\") is None:\n",
    "            job_application = \"Error\"\n",
    "        else:\n",
    "            job_application = job_soup.find(\"span\",{\"class\":\"jApplys\"}).find(\"strong\").getText().strip()\n",
    "    if job_soup.find(\"span\",{\"class\":\"jViews\"}) is None:\n",
    "        job_view = \"Error\"\n",
    "    else:\n",
    "        if job_soup.find(\"span\",{\"class\":\"jViews\"}).find(\"strong\") is None:\n",
    "            job_view = \"Error\"\n",
    "        else:\n",
    "            job_view = job_soup.find(\"span\",{\"class\":\"jViews\"}).find(\"strong\").getText().strip()\n",
    "    if job_soup.find(\"ul\",{\"itemprop\":\"description\"}) is None:\n",
    "        job_description = \"Error\"\n",
    "    else:\n",
    "        job_description = job_soup.find(\"ul\",{\"itemprop\":\"description\"}).getText().strip()\n",
    "    # description labels (other informations about the job)\n",
    "    #for x in job_soup.find(\"div\",{\"class\":\"jDisc mt20\"}).contents:\n",
    "    #    if len(str(x).replace(' ',''))!=0 :\n",
    "    #        print(x.getText().split(':')[-1].strip())\n",
    "    if job_soup.find(\"div\",{\"class\":\"jDisc mt20\"}) is None:\n",
    "        other_info_label = {}\n",
    "    else:\n",
    "        other_info = [x.getText().split(':')[-1].strip() for x in job_soup.find(\"div\",{\"class\":\"jDisc mt20\"}).contents if len(str(x).replace(' ',''))!=0]\n",
    "        other_info_label = {labels: other_info for labels, other_info in zip(labels, other_info)}\n",
    "    if job_soup.find(\"div\",{\"class\":\"ksTags\"}) is None:\n",
    "        key_skills = \"Error\"\n",
    "    else:\n",
    "        key_skills = ','.join(job_soup.find(\"div\",{\"class\":\"ksTags\"}).getText().split(\"  \"))[1:]\n",
    "    if job_soup.find(\"ul\",{\"class\":\"listing mt15\"}) is None:\n",
    "        skill_experience = \"Error\"\n",
    "    else:\n",
    "        skill_experience = job_soup.find(\"ul\",{\"class\":\"listing mt15\"}).getText().strip()\n",
    "    # putting the education information\n",
    "    if job_soup.find(\"div\",{\"itemprop\":\"educationRequirements\"}) is None:\n",
    "        education_info = {}\n",
    "        for l in edu_labels:\n",
    "            education_info[l] = \"\"\n",
    "    else:\n",
    "        education = [x.getText().split(':') for x in job_soup.find(\"div\",{\"itemprop\":\"educationRequirements\"}).contents if len(str(x).replace(' ',''))!=0]\n",
    "        education_info = {edu_label.strip(): education.strip() for edu_label, education in education}\n",
    "        for l in edu_labels:\n",
    "            if l not in education_info.keys():\n",
    "                education_info[l] = \"\"\n",
    "    # recruiter information\n",
    "    # This is inside a javascript button : BeautifulSoup is HTML parser: For this need SELENIUM\n",
    "    #except AttributeError:\n",
    "    #    print(\"Attribute Error\")\n",
    "    #    continue\n",
    "    df = OrderedDict({'Job_Link':job_url, 'Job_Title':job_title, 'Company_Name':company_name, 'Experience':experience, 'Location':location, \n",
    "                      'SalaryI':salary, 'Num_Openings':num_openings, 'Job_Post':job_post, 'Job_Application':job_application, 'Job_View':job_view, \n",
    "                      'Key_Skills':key_skills, 'Skill_Experience':skill_experience})\n",
    "    df.update(other_info_label)\n",
    "    df.update(education_info)\n",
    "    job_rem_df1 = job_rem_df1.append(df,ignore_index=True)\n",
    "    print(job_rem_df1.shape)\n",
    "    time.sleep(1)\n",
    "print(\"Iteration Completed and get data of\" + str(job_rem_df1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Design_Role</th>\n",
       "      <th>Doctorate</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Functional_Area</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Job_Application</th>\n",
       "      <th>Job_Link</th>\n",
       "      <th>Job_Post</th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_View</th>\n",
       "      <th>Key_Skills</th>\n",
       "      <th>Location</th>\n",
       "      <th>Num_Openings</th>\n",
       "      <th>PG</th>\n",
       "      <th>Role_Category</th>\n",
       "      <th>Salary</th>\n",
       "      <th>SalaryI</th>\n",
       "      <th>Skill_Experience</th>\n",
       "      <th>UG</th>\n",
       "      <th>Actual_Job_Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wissen Infotech Pvt. Ltd.</td>\n",
       "      <td>Permanent Job, Full Time</td>\n",
       "      <td></td>\n",
       "      <td>5 - 8 yrs</td>\n",
       "      <td>Other</td>\n",
       "      <td>IT-Software  /    Software Services</td>\n",
       "      <td>64</td>\n",
       "      <td>https://www.naukri.com/job-listings-AI-Scienti...</td>\n",
       "      <td>Posted 8 days ago</td>\n",
       "      <td>AI Scientist</td>\n",
       "      <td>583</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bengaluru(Marathahalli)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Other</td>\n",
       "      <td>Not Disclosed by Recruiter</td>\n",
       "      <td>Not Disclosed by Recruiter</td>\n",
       "      <td>Please refer to the Job description above</td>\n",
       "      <td></td>\n",
       "      <td>https://www.naukri.com/job-listings-AI-Scienti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Error</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Error</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44</td>\n",
       "      <td>https://www.naukri.com/job-listings-Senior-Dat...</td>\n",
       "      <td>Posted: 1 day ago</td>\n",
       "      <td>Error</td>\n",
       "      <td>28</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Disclosed by Recruiter</td>\n",
       "      <td>Error</td>\n",
       "      <td></td>\n",
       "      <td>https://www.naukri.com/job-listings-Senior-Dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Red Hat India Pvt Ltd</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td></td>\n",
       "      <td>7 - 12 yrs</td>\n",
       "      <td>Analytics &amp; Business Intelligence</td>\n",
       "      <td>IT-Software  /    Software Services</td>\n",
       "      <td>Less than 10</td>\n",
       "      <td>https://www.naukri.com/job-listings-Business-D...</td>\n",
       "      <td>Posted 2 days ago</td>\n",
       "      <td>Business Data Scientist _ Redhat_pune</td>\n",
       "      <td>192</td>\n",
       "      <td>sql queries,amazon aws,redshift,data modeling,...</td>\n",
       "      <td>Pune</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Analytics &amp; BI</td>\n",
       "      <td>INR  8,00,000 - 13,00,000 P.A.</td>\n",
       "      <td>8,00,000 - 13,00,000 P.A.</td>\n",
       "      <td>Please refer to the Job description above</td>\n",
       "      <td></td>\n",
       "      <td>https://www.naukri.com/job-listings-Business-D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Company_Name               Design_Role Doctorate  Experience                    Functional_Area                             Industry Job_Application                                           Job_Link           Job_Post                              Job_Title Job_View                                         Key_Skills                 Location Num_Openings PG   Role_Category                          Salary                     SalaryI                           Skill_Experience UG  \\\n",
       "0  Wissen Infotech Pvt. Ltd.  Permanent Job, Full Time             5 - 8 yrs                              Other  IT-Software  /    Software Services              64  https://www.naukri.com/job-listings-AI-Scienti...  Posted 8 days ago                           AI Scientist      583                                    Data Scientist   Bengaluru(Marathahalli)                           Other      Not Disclosed by Recruiter  Not Disclosed by Recruiter  Please refer to the Job description above      \n",
       "1                      Error                       NaN                 Error                                NaN                                  NaN              44  https://www.naukri.com/job-listings-Senior-Dat...  Posted: 1 day ago                                  Error       28                                              Error                    Error                             NaN                             NaN  Not Disclosed by Recruiter                                      Error      \n",
       "2      Red Hat India Pvt Ltd              Data Analyst            7 - 12 yrs  Analytics & Business Intelligence  IT-Software  /    Software Services    Less than 10  https://www.naukri.com/job-listings-Business-D...  Posted 2 days ago  Business Data Scientist _ Redhat_pune      192  sql queries,amazon aws,redshift,data modeling,...                     Pune                  Analytics & BI  INR  8,00,000 - 13,00,000 P.A.   8,00,000 - 13,00,000 P.A.  Please refer to the Job description above      \n",
       "\n",
       "                                     Actual_Job_Link  \n",
       "0  https://www.naukri.com/job-listings-AI-Scienti...  \n",
       "1  https://www.naukri.com/job-listings-Senior-Dat...  \n",
       "2  https://www.naukri.com/job-listings-Business-D...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_rem_df1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of newly scrap job (2351, 21)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(557, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add Actual_Job_Link column\n",
    "job_rem_df1[\"Actual_Job_Link\"] = job_rem_df1[\"Job_Link\"]\n",
    "\n",
    "# Now we have 2908 rows df, but in many places Whole row is as Error or None\n",
    "\n",
    "# Filter the valid data\n",
    "valid_job = job_rem_df1[job_rem_df1[\"Company_Name\"] != \"Error\"]\n",
    "print(\"Size of newly scrap job \" + str(valid_job.shape))\n",
    "\n",
    "# Append with job_df\n",
    "job_df = job_df.append(valid_job)\n",
    "\n",
    "job_df.to_csv(\"complete_job_profiles.csv\")\n",
    "\n",
    "# Page with different structure\n",
    "invalid_jobs = job_rem_df1[job_rem_df1[\"Company_Name\"] == \"Error\"]\n",
    "invalid_jobs.to_csv(\"invalid_jobs.csv\")\n",
    "dif_links = invalid_jobs[\"Actual_Job_Link\"].to_frame()\n",
    "dif_links.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Design_Role</th>\n",
       "      <th>Doctorate</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Functional_Area</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Job_Application</th>\n",
       "      <th>Job_Link</th>\n",
       "      <th>Job_Post</th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_View</th>\n",
       "      <th>Key_Skills</th>\n",
       "      <th>Location</th>\n",
       "      <th>Num_Openings</th>\n",
       "      <th>PG</th>\n",
       "      <th>Role_Category</th>\n",
       "      <th>Salary</th>\n",
       "      <th>SalaryI</th>\n",
       "      <th>Skill_Experience</th>\n",
       "      <th>UG</th>\n",
       "      <th>Actual_Job_Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wissen Infotech Pvt. Ltd.</td>\n",
       "      <td>Permanent Job, Full Time</td>\n",
       "      <td></td>\n",
       "      <td>5 - 8 yrs</td>\n",
       "      <td>Other</td>\n",
       "      <td>IT-Software  /    Software Services</td>\n",
       "      <td>64</td>\n",
       "      <td>https://www.naukri.com/job-listings-AI-Scienti...</td>\n",
       "      <td>Posted 8 days ago</td>\n",
       "      <td>AI Scientist</td>\n",
       "      <td>583</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bengaluru(Marathahalli)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Other</td>\n",
       "      <td>Not Disclosed by Recruiter</td>\n",
       "      <td>Not Disclosed by Recruiter</td>\n",
       "      <td>Please refer to the Job description above</td>\n",
       "      <td></td>\n",
       "      <td>https://www.naukri.com/job-listings-AI-Scienti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Red Hat India Pvt Ltd</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td></td>\n",
       "      <td>7 - 12 yrs</td>\n",
       "      <td>Analytics &amp; Business Intelligence</td>\n",
       "      <td>IT-Software  /    Software Services</td>\n",
       "      <td>Less than 10</td>\n",
       "      <td>https://www.naukri.com/job-listings-Business-D...</td>\n",
       "      <td>Posted 2 days ago</td>\n",
       "      <td>Business Data Scientist _ Redhat_pune</td>\n",
       "      <td>192</td>\n",
       "      <td>sql queries,amazon aws,redshift,data modeling,...</td>\n",
       "      <td>Pune</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Analytics &amp; BI</td>\n",
       "      <td>INR  8,00,000 - 13,00,000 P.A.</td>\n",
       "      <td>8,00,000 - 13,00,000 P.A.</td>\n",
       "      <td>Please refer to the Job description above</td>\n",
       "      <td></td>\n",
       "      <td>https://www.naukri.com/job-listings-Business-D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adecco India Private Limited</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td></td>\n",
       "      <td>3 - 5 yrs</td>\n",
       "      <td>Analytics &amp; Business Intelligence</td>\n",
       "      <td>IT-Software  /    Software Services</td>\n",
       "      <td>742</td>\n",
       "      <td>https://www.naukri.com/job-listings-Data-Scien...</td>\n",
       "      <td>Posted 2 days ago</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>1320</td>\n",
       "      <td>SAS SQL,R,Python,Excel,Analytics,Data Manipula...</td>\n",
       "      <td>Pune</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Analytics &amp; BI</td>\n",
       "      <td>Not Disclosed by Recruiter</td>\n",
       "      <td>Not Disclosed by Recruiter</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://www.naukri.com/job-listings-Data-Scien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Company_Name               Design_Role Doctorate  Experience                    Functional_Area                             Industry Job_Application                                           Job_Link           Job_Post                              Job_Title Job_View                                         Key_Skills                 Location Num_Openings PG   Role_Category                          Salary                     SalaryI                           Skill_Experience  \\\n",
       "0     Wissen Infotech Pvt. Ltd.  Permanent Job, Full Time             5 - 8 yrs                              Other  IT-Software  /    Software Services              64  https://www.naukri.com/job-listings-AI-Scienti...  Posted 8 days ago                           AI Scientist      583                                    Data Scientist   Bengaluru(Marathahalli)                           Other      Not Disclosed by Recruiter  Not Disclosed by Recruiter  Please refer to the Job description above   \n",
       "2         Red Hat India Pvt Ltd              Data Analyst            7 - 12 yrs  Analytics & Business Intelligence  IT-Software  /    Software Services    Less than 10  https://www.naukri.com/job-listings-Business-D...  Posted 2 days ago  Business Data Scientist _ Redhat_pune      192  sql queries,amazon aws,redshift,data modeling,...                     Pune                  Analytics & BI  INR  8,00,000 - 13,00,000 P.A.   8,00,000 - 13,00,000 P.A.  Please refer to the Job description above   \n",
       "3  Adecco India Private Limited              Data Analyst             3 - 5 yrs  Analytics & Business Intelligence  IT-Software  /    Software Services             742  https://www.naukri.com/job-listings-Data-Scien...  Posted 2 days ago                         Data Scientist     1320  SAS SQL,R,Python,Excel,Analytics,Data Manipula...                     Pune                  Analytics & BI      Not Disclosed by Recruiter  Not Disclosed by Recruiter                                              \n",
       "\n",
       "  UG                                    Actual_Job_Link  \n",
       "0     https://www.naukri.com/job-listings-AI-Scienti...  \n",
       "2     https://www.naukri.com/job-listings-Business-D...  \n",
       "3     https://www.naukri.com/job-listings-Data-Scien...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_job.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaining list 557: We will leave these\n",
    "#### Iterate though the new list\n",
    "#### Initialize a new df\n",
    "job_dl_df = pd.DataFrame()\n",
    "\n",
    "#### description labels (other informations about the job)\n",
    "labels = ['Salary', 'Industry', 'Functional_Area', 'Role_Category', 'Design_Role']\n",
    "#### education requirements\n",
    "edu_labels = [\"UG\", \"PG\", \"Doctorate\"]\n",
    "\n",
    "#### Iterate through all the links in remaining_job_list\n",
    "for index, row in dif_links.iterrows():\n",
    "    job_url = row[\"Actual_Job_Link\"]\n",
    "    job_response = requests.get(job_url)\n",
    "    job_page = job_response.text\n",
    "    job_soup = BeautifulSoup(job_page, \"html.parser\")    \n",
    "    #try:\n",
    "    if job_soup.find(\"h1\", {\"itemprop\":\"title\"}) is None:\n",
    "        job_title = \"Error\"\n",
    "    else:\n",
    "        job_title = job_soup.find(\"h1\", {\"itemprop\":\"title\"}).getText().strip()\n",
    "    if job_soup.find(\"a\",{\"itemprop\":\"hiringOrganization\"}) is None:\n",
    "        company_name = \"Error\"\n",
    "    else:\n",
    "        company_name = job_soup.find(\"a\",{\"itemprop\":\"hiringOrganization\"}).getText().strip()\n",
    "    if job_soup.find(\"span\",{\"itemprop\":\"experienceRequirements\"}) is None:\n",
    "        experience = \"Error\"\n",
    "    else:\n",
    "        experience = job_soup.find(\"span\",{\"itemprop\":\"experienceRequirements\"}).getText().strip()\n",
    "    if job_soup.find(\"div\",{\"class\":\"loc\"}) is None:\n",
    "        location = \"Error\"\n",
    "    else:\n",
    "        location = job_soup.find(\"div\",{\"class\":\"loc\"}).getText().strip()\n",
    "    if job_soup.find(\"span\",{\"class\":\"sal\"}) is None:\n",
    "        salary = \"Error\"\n",
    "    else:\n",
    "        salary = job_soup.find(\"span\",{\"class\":\"sal\"}).getText().strip()\n",
    "    #openings = job_soup.find(\"div\",{\"class\":\"sumFoot\"}).find_all(\"span\")\n",
    "    #openings\n",
    "    num_openings = \"\"\n",
    "    job_post = \"\"\n",
    "    if job_soup.find(\"div\",{\"class\":\"sumFoot\"}) is None:\n",
    "        num_openings = \"Error\"\n",
    "        job_post = \"Error\"\n",
    "    else:\n",
    "        if job_soup.find(\"div\",{\"class\":\"sumFoot\"}).find_all(\"span\") is None:\n",
    "            num_openings = \"Error\"\n",
    "            job_post = \"Error\"\n",
    "        else:\n",
    "            for x in job_soup.find(\"div\",{\"class\":\"sumFoot\"}).find_all(\"span\"):\n",
    "                if \"Openings\" in x.text.strip():\n",
    "                    num_openings = x.text.strip()\n",
    "                if \"Posted\" in x.text.strip():\n",
    "                    job_post = x.text.strip()\n",
    "    if job_soup.find(\"span\",{\"class\":\"jApplys\"}) is None:\n",
    "        job_application = \"Error\"\n",
    "    else:\n",
    "        if job_soup.find(\"span\",{\"class\":\"jApplys\"}).find(\"strong\") is None:\n",
    "            job_application = \"Error\"\n",
    "        else:\n",
    "            job_application = job_soup.find(\"span\",{\"class\":\"jApplys\"}).find(\"strong\").getText().strip()\n",
    "    if job_soup.find(\"span\",{\"class\":\"jViews\"}) is None:\n",
    "        job_view = \"Error\"\n",
    "    else:\n",
    "        if job_soup.find(\"span\",{\"class\":\"jViews\"}).find(\"strong\") is None:\n",
    "            job_view = \"Error\"\n",
    "        else:\n",
    "            job_view = job_soup.find(\"span\",{\"class\":\"jViews\"}).find(\"strong\").getText().strip()\n",
    "    if job_soup.find(\"ul\",{\"itemprop\":\"description\"}) is None:\n",
    "        job_description = \"Error\"\n",
    "    else:\n",
    "        job_description = job_soup.find(\"ul\",{\"itemprop\":\"description\"}).getText().strip()\n",
    "    #description labels (other informations about the job)\n",
    "    #for x in job_soup.find(\"div\",{\"class\":\"jDisc mt20\"}).contents:\n",
    "    #if len(str(x).replace(' ',''))!=0 :\n",
    "    #print(x.getText().split(':')[-1].strip())\n",
    "    if job_soup.find(\"div\",{\"class\":\"jDisc mt20\"}) is None:\n",
    "        other_info_label = {}\n",
    "    else:\n",
    "        other_info = [x.getText().split(':')[-1].strip() for x in job_soup.find(\"div\",{\"class\":\"jDisc mt20\"}).contents if len(str(x).replace(' ',''))!=0]\n",
    "        other_info_label = {labels: other_info for labels, other_info in zip(labels, other_info)}\n",
    "    if job_soup.find(\"div\",{\"class\":\"ksTags\"}) is None:\n",
    "        key_skills = \"Error\"\n",
    "    else:\n",
    "        key_skills = ','.join(job_soup.find(\"div\",{\"class\":\"ksTags\"}).getText().split(\"  \"))[1:]\n",
    "    if job_soup.find(\"ul\",{\"class\":\"listing mt15\"}) is None:\n",
    "        skill_experience = \"Error\"\n",
    "    else:\n",
    "        skill_experience = job_soup.find(\"ul\",{\"class\":\"listing mt15\"}).getText().strip()\n",
    "    #putting the education information\n",
    "    if job_soup.find(\"div\",{\"itemprop\":\"educationRequirements\"}) is None:\n",
    "        education_info = {}\n",
    "        for l in edu_labels:\n",
    "            education_info[l] = \"\"\n",
    "    else:\n",
    "        education = [x.getText().split(':') for x in job_soup.find(\"div\",{\"itemprop\":\"educationRequirements\"}).contents if len(str(x).replace(' ',''))!=0]\n",
    "        education_info = {edu_label.strip(): education.strip() for edu_label, education in education}\n",
    "        for l in edu_labels:\n",
    "            if l not in education_info.keys():\n",
    "                education_info[l] = \"\"\n",
    "    #recruiter information\n",
    "    #This is inside a javascript button : BeautifulSoup is HTML parser: For this need SELENIUM\n",
    "    #except AttributeError:\n",
    "    #print(\"Attribute Error\")\n",
    "    #continue\n",
    "    df = OrderedDict({'Job_Link':job_url, 'Job_Title':job_title, 'Company_Name':company_name, 'Experience':experience, 'Location':location, \n",
    "                      'SalaryI':salary, 'Num_Openings':num_openings, 'Job_Post':job_post, 'Job_Application':job_application, 'Job_View':job_view, \n",
    "                      'Key_Skills':key_skills, 'Skill_Experience':skill_experience})\n",
    "    df.update(other_info_label)\n",
    "    df.update(education_info)\n",
    "    job_dl_df = job_dl_df.append(df,ignore_index=True)\n",
    "    print(job_dl_df.shape)\n",
    "    time.sleep(1)\n",
    "print(\"Iteration Completed and get data of\" + str(job_dl_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total jobs we scrapped is 6098\n"
     ]
    }
   ],
   "source": [
    "print(\"Total jobs we scrapped is \" + str(job_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save us all the job profiles as pkl as well as csv file\n",
    "with open(\"data/complete_job_profiles.pkl\", \"wb\") as f:\n",
    "    cPickle.dump(job_df, f)\n",
    "    \n",
    "job_df.to_csv(\" data/complete_job_profiles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The next work will be cleaning the data for further analysis :**    \n",
    "> We will perform the task in some other notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
